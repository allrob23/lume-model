{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LUME-model","text":"<p>LUME-model holds data structures used in the LUME modeling toolset. Variables and models built using LUME-model will be compatible with other tools. LUME-model uses Pydantic models to enforce typed attributes upon assignment.</p>"},{"location":"#installing-lume-model","title":"Installing LUME-model","text":"<p>LUME-model can be installed with conda using the command: <pre><code>conda install lume-model -c conda-forge\n</code></pre></p>"},{"location":"#variables","title":"Variables","text":"<p>The lume-model variables are intended to enforce requirements for input and output variables by variable type. For now, only scalar variables (floats) are supported.</p> <p>Minimal example of scalar input and output variables:</p> <pre><code>from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n\ninput_variable = ScalarInputVariable(\n    name=\"example_input\",\n    default=0.1,\n    value_range=[0.0, 1.0],\n)\noutput_variable = ScalarOutputVariable(name=\"example_output\")\n</code></pre>"},{"location":"#models","title":"Models","text":"<p>The lume-model base class <code>lume_model.base.LUMEBaseModel</code> is intended to guide user development while allowing for flexibility and customizability. It is used to enforce LUME tool compatible classes for the execution of trained models.</p> <p>Requirements for model classes:</p> <ul> <li>input_variables: A list defining the input variables for the model. Variable names must be unique. Required for use with lume-epics tools.</li> <li>output_variables: A list defining the output variables for the model. Variable names must be unique. Required for use with lume-epics tools.</li> <li>evaluate: The evaluate method is called by the serving model. Subclasses must implement this method, accepting and returning a dictionary.</li> </ul> <p>Example model implementation and instantiation:</p> <pre><code>from lume_model.base import LUMEBaseModel\nfrom lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n\n\nclass ExampleModel(LUMEBaseModel):\n    def evaluate(self, input_dict):\n        output_dict = {\n            \"output1\": input_dict[self.input_variables[0].name] ** 2,\n            \"output2\": input_dict[self.input_variables[1].name] ** 2,\n        }\n        return output_dict\n\n\ninput_variables = [\n    ScalarInputVariable(name=\"input1\", default=0.1, value_range=[0.0, 1.0]),\n    ScalarInputVariable(name=\"input2\", default=0.2, value_range=[0.0, 1.0]),\n]\noutput_variables = [\n    ScalarOutputVariable(name=\"output1\"),\n    ScalarOutputVariable(name=\"output2\"),\n]\n\nm = ExampleModel(input_variables=input_variables, output_variables=output_variables)\n</code></pre> <p>Models and variables can be saved and loaded from YAML files, e.g. <code>m.dump(\"example_model.yml\")</code> writes the following to file</p> <pre><code>model_class: ExampleModel\ninput_variables:\n  input1:\n    variable_type: scalar\n    default: 0.1\n    is_constant: false\n    value_range: [0.0, 1.0]\n  input2:\n    variable_type: scalar\n    default: 0.2\n    is_constant: false\n    value_range: [0.0, 1.0]\noutput_variables:\n  output1: {variable_type: scalar}\n  output2: {variable_type: scalar}\n</code></pre> <p>and can be loaded by simply passing the file to the model constructor:</p> <pre><code>from lume_model.base import LUMEBaseModel\n\n\nclass ExampleModel(LUMEBaseModel):\n    def evaluate(self, input_dict):\n        output_dict = {\n            \"output1\": input_dict[self.input_variables[0].name] ** 2,\n            \"output2\": input_dict[self.input_variables[1].name] ** 2,\n        }\n        return output_dict\n\n\nm = ExampleModel(\"example_model.yml\")\n</code></pre>"},{"location":"#developer","title":"Developer","text":"<p>Clone this repository: <pre><code>git clone https://github.com/slaclab/lume-model.git\n</code></pre></p> <p>Create an environment lume-model-dev with all the dependencies: <pre><code>conda env create -f dev-environment.yml\n</code></pre></p> <p>Install as editable: <pre><code>conda activate lume-model-dev\npip install --no-dependencies -e .\n</code></pre></p>"},{"location":"models/","title":"Models","text":""},{"location":"models/#lume_model.base.LUMEBaseModel","title":"<code>LUMEBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for models using lume-model variables.</p> <p>Inheriting classes must define the evaluate method and variable names must be unique (respectively). Models build using this framework will be compatible with the lume-epics EPICS server and associated tools.</p> <p>Attributes:</p> Name Type Description <code>input_variables</code> <code>list[SerializeAsAny[InputVariable]]</code> <p>List defining the input variables and their order.</p> <code>output_variables</code> <code>list[SerializeAsAny[OutputVariable]]</code> <p>List defining the output variables and their order.</p> Source code in <code>lume_model/base.py</code> <pre><code>class LUMEBaseModel(BaseModel, ABC):\n    \"\"\"Abstract base class for models using lume-model variables.\n\n    Inheriting classes must define the evaluate method and variable names must be unique (respectively).\n    Models build using this framework will be compatible with the lume-epics EPICS server and associated tools.\n\n    Attributes:\n        input_variables: List defining the input variables and their order.\n        output_variables: List defining the output variables and their order.\n    \"\"\"\n    input_variables: list[SerializeAsAny[InputVariable]]\n    output_variables: list[SerializeAsAny[OutputVariable]]\n\n    model_config = ConfigDict(arbitrary_types_allowed=True, validate_assignment=True)\n\n    @field_validator(\"input_variables\", mode=\"before\")\n    def validate_input_variables(cls, value):\n        new_value = []\n        if isinstance(value, dict):\n            for name, val in value.items():\n                if isinstance(val, dict):\n                    if val[\"variable_type\"] == \"scalar\":\n                        new_value.append(ScalarInputVariable(name=name, **val))\n                elif isinstance(val, InputVariable):\n                    new_value.append(val)\n                else:\n                    raise TypeError(f\"type {type(val)} not supported\")\n        elif isinstance(value, list):\n            new_value = value\n\n        return new_value\n\n    @field_validator(\"output_variables\", mode=\"before\")\n    def validate_output_variables(cls, value):\n        new_value = []\n        if isinstance(value, dict):\n            for name, val in value.items():\n                if isinstance(val, dict):\n                    if val[\"variable_type\"] == \"scalar\":\n                        new_value.append(ScalarOutputVariable(name=name, **val))\n                elif isinstance(val, OutputVariable):\n                    new_value.append(val)\n                else:\n                    raise TypeError(f\"type {type(val)} not supported\")\n        elif isinstance(value, list):\n            new_value = value\n\n        return new_value\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes LUMEBaseModel.\n\n        Args:\n            *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n              formatted string or file path.\n            **kwargs: See class attributes.\n        \"\"\"\n        if len(args) == 1:\n            if len(kwargs) &gt; 0:\n                raise ValueError(\"Cannot specify YAML string and keyword arguments for LUMEBaseModel init.\")\n            super().__init__(**parse_config(args[0], self.model_fields))\n        elif len(args) &gt; 1:\n            raise ValueError(\n                \"Arguments to LUMEBaseModel must be either a single YAML string \"\n                \"or keyword arguments passed directly to pydantic.\"\n            )\n        else:\n            super().__init__(**kwargs)\n\n    @field_validator(\"input_variables\", \"output_variables\")\n    def unique_variable_names(cls, value):\n        verify_unique_variable_names(value)\n        return value\n\n    @property\n    def input_names(self) -&gt; list[str]:\n        return [var.name for var in self.input_variables]\n\n    @property\n    def output_names(self) -&gt; list[str]:\n        return [var.name for var in self.output_variables]\n\n    @abstractmethod\n    def evaluate(self, input_dict: dict[str, Any]) -&gt; dict[str, Any]:\n        pass\n\n    def to_json(self, **kwargs) -&gt; str:\n        return json_dumps(self, **kwargs)\n\n    def dict(self, **kwargs) -&gt; dict[str, Any]:\n        config = super().model_dump(**kwargs)\n        return {\"model_class\": self.__class__.__name__} | config\n\n    def json(self, **kwargs) -&gt; str:\n        result = self.to_json(**kwargs)\n        config = json.loads(result)\n        config = {\"model_class\": self.__class__.__name__} | config\n        return json.dumps(config)\n\n    def yaml(\n            self,\n            base_key: str = \"\",\n            file_prefix: str = \"\",\n            save_models: bool = False,\n    ) -&gt; str:\n        \"\"\"Serializes the object and returns a YAML formatted string defining the model.\n\n        Args:\n            base_key: Base key for serialization.\n            file_prefix: Prefix for generated filenames.\n            save_models: Determines whether models are saved to file.\n\n        Returns:\n            YAML formatted string defining the model.\n        \"\"\"\n        output = json.loads(\n            self.to_json(\n                base_key=base_key,\n                file_prefix=file_prefix,\n                save_models=save_models,\n            )\n        )\n        s = yaml.dump({\"model_class\": self.__class__.__name__} | output,\n                      default_flow_style=None, sort_keys=False)\n        return s\n\n    def dump(\n            self,\n            file: Union[str, os.PathLike],\n            base_key: str = \"\",\n            save_models: bool = True,\n    ):\n        \"\"\"Returns and optionally saves YAML formatted string defining the model.\n\n        Args:\n            file: File path to which the YAML formatted string and corresponding files are saved.\n            base_key: Base key for serialization.\n            save_models: Determines whether models are saved to file.\n        \"\"\"\n        file_prefix = os.path.splitext(os.path.abspath(file))[0]\n        with open(file, \"w\") as f:\n            f.write(\n                self.yaml(\n                    base_key=base_key,\n                    file_prefix=file_prefix,\n                    save_models=save_models,\n                )\n            )\n\n    @classmethod\n    def from_file(cls, filename: str):\n        if not os.path.exists(filename):\n            raise OSError(f\"File {filename} is not found.\")\n        with open(filename, \"r\") as file:\n            return cls.from_yaml(file)\n\n    @classmethod\n    def from_yaml(cls, yaml_obj: [str, TextIOWrapper]):\n        return cls.model_validate(parse_config(yaml_obj, cls.model_fields))\n</code></pre>"},{"location":"models/#lume_model.base.LUMEBaseModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes LUMEBaseModel.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Accepts a single argument which is the model configuration as dictionary, YAML or JSON formatted string or file path.</p> <code>()</code> <code>**kwargs</code> <p>See class attributes.</p> <code>{}</code> Source code in <code>lume_model/base.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes LUMEBaseModel.\n\n    Args:\n        *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n          formatted string or file path.\n        **kwargs: See class attributes.\n    \"\"\"\n    if len(args) == 1:\n        if len(kwargs) &gt; 0:\n            raise ValueError(\"Cannot specify YAML string and keyword arguments for LUMEBaseModel init.\")\n        super().__init__(**parse_config(args[0], self.model_fields))\n    elif len(args) &gt; 1:\n        raise ValueError(\n            \"Arguments to LUMEBaseModel must be either a single YAML string \"\n            \"or keyword arguments passed directly to pydantic.\"\n        )\n    else:\n        super().__init__(**kwargs)\n</code></pre>"},{"location":"models/#lume_model.base.LUMEBaseModel.dump","title":"<code>dump(file, base_key='', save_models=True)</code>","text":"<p>Returns and optionally saves YAML formatted string defining the model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, PathLike]</code> <p>File path to which the YAML formatted string and corresponding files are saved.</p> required <code>base_key</code> <code>str</code> <p>Base key for serialization.</p> <code>''</code> <code>save_models</code> <code>bool</code> <p>Determines whether models are saved to file.</p> <code>True</code> Source code in <code>lume_model/base.py</code> <pre><code>def dump(\n        self,\n        file: Union[str, os.PathLike],\n        base_key: str = \"\",\n        save_models: bool = True,\n):\n    \"\"\"Returns and optionally saves YAML formatted string defining the model.\n\n    Args:\n        file: File path to which the YAML formatted string and corresponding files are saved.\n        base_key: Base key for serialization.\n        save_models: Determines whether models are saved to file.\n    \"\"\"\n    file_prefix = os.path.splitext(os.path.abspath(file))[0]\n    with open(file, \"w\") as f:\n        f.write(\n            self.yaml(\n                base_key=base_key,\n                file_prefix=file_prefix,\n                save_models=save_models,\n            )\n        )\n</code></pre>"},{"location":"models/#lume_model.base.LUMEBaseModel.yaml","title":"<code>yaml(base_key='', file_prefix='', save_models=False)</code>","text":"<p>Serializes the object and returns a YAML formatted string defining the model.</p> <p>Parameters:</p> Name Type Description Default <code>base_key</code> <code>str</code> <p>Base key for serialization.</p> <code>''</code> <code>file_prefix</code> <code>str</code> <p>Prefix for generated filenames.</p> <code>''</code> <code>save_models</code> <code>bool</code> <p>Determines whether models are saved to file.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>YAML formatted string defining the model.</p> Source code in <code>lume_model/base.py</code> <pre><code>def yaml(\n        self,\n        base_key: str = \"\",\n        file_prefix: str = \"\",\n        save_models: bool = False,\n) -&gt; str:\n    \"\"\"Serializes the object and returns a YAML formatted string defining the model.\n\n    Args:\n        base_key: Base key for serialization.\n        file_prefix: Prefix for generated filenames.\n        save_models: Determines whether models are saved to file.\n\n    Returns:\n        YAML formatted string defining the model.\n    \"\"\"\n    output = json.loads(\n        self.to_json(\n            base_key=base_key,\n            file_prefix=file_prefix,\n            save_models=save_models,\n        )\n    )\n    s = yaml.dump({\"model_class\": self.__class__.__name__} | output,\n                  default_flow_style=None, sort_keys=False)\n    return s\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel","title":"<code>TorchModel</code>","text":"<p>               Bases: <code>LUMEBaseModel</code></p> <p>LUME-model class for torch models.</p> <p>By default, the models are assumed to be fixed, so all gradient computation is deactivated and the model and transformers are put in evaluation mode.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>The torch base model.</p> <code>input_variables</code> <code>Module</code> <p>List defining the input variables and their order.</p> <code>output_variables</code> <code>Module</code> <p>List defining the output variables and their order.</p> <code>input_transformers</code> <code>list[ReversibleInputTransform]</code> <p>List of transformer objects to apply to input before passing to model.</p> <code>output_transformers</code> <code>list[ReversibleInputTransform]</code> <p>List of transformer objects to apply to output of model.</p> <code>output_format</code> <code>str</code> <p>Determines format of outputs: \"tensor\", \"variable\" or \"raw\".</p> <code>device</code> <code>Union[device, str]</code> <p>Device on which the model will be evaluated. Defaults to \"cpu\".</p> <code>fixed_model</code> <code>bool</code> <p>If true, the model and transformers are put in evaluation mode and all gradient computation is deactivated.</p> Source code in <code>lume_model/models/torch_model.py</code> <pre><code>class TorchModel(LUMEBaseModel):\n    \"\"\"LUME-model class for torch models.\n\n    By default, the models are assumed to be fixed, so all gradient computation is deactivated and the model and\n    transformers are put in evaluation mode.\n\n    Attributes:\n        model: The torch base model.\n        input_variables: List defining the input variables and their order.\n        output_variables: List defining the output variables and their order.\n        input_transformers: List of transformer objects to apply to input before passing to model.\n        output_transformers: List of transformer objects to apply to output of model.\n        output_format: Determines format of outputs: \"tensor\", \"variable\" or \"raw\".\n        device: Device on which the model will be evaluated. Defaults to \"cpu\".\n        fixed_model: If true, the model and transformers are put in evaluation mode and all gradient\n          computation is deactivated.\n    \"\"\"\n    model: torch.nn.Module\n    input_transformers: list[ReversibleInputTransform] = []\n    output_transformers: list[ReversibleInputTransform] = []\n    output_format: str = \"tensor\"\n    device: Union[torch.device, str] = \"cpu\"\n    fixed_model: bool = True\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes TorchModel.\n\n        Args:\n            *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n              formatted string or file path.\n            **kwargs: See class attributes.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # set precision\n        self.model.to(dtype=self.dtype)\n        for t in self.input_transformers + self.output_transformers:\n            if isinstance(t, torch.nn.Module):\n                t.to(dtype=self.dtype)\n\n        # fixed model: set full model in eval mode and deactivate all gradients\n        if self.fixed_model:\n            self.model.eval().requires_grad_(False)\n            for t in self.input_transformers + self.output_transformers:\n                if isinstance(t, torch.nn.Module):\n                    t.eval().requires_grad_(False)\n\n        # ensure consistent device\n        self.to(self.device)\n\n    @property\n    def dtype(self):\n        return torch.double\n\n    @property\n    def _tkwargs(self):\n        return {\"device\": self.device, \"dtype\": self.dtype}\n\n    @field_validator(\"model\", mode=\"before\")\n    def validate_torch_model(cls, v):\n        if isinstance(v, (str, os.PathLike)):\n            if os.path.exists(v):\n                v = torch.load(v)\n            else:\n                raise OSError(f\"File {v} is not found.\")\n        return v\n\n    @field_validator(\"input_transformers\", \"output_transformers\", mode=\"before\")\n    def validate_botorch_transformers(cls, v):\n        if not isinstance(v, list):\n            raise ValueError(\"Transformers must be passed as list.\")\n        loaded_transformers = []\n        for t in v:\n            if isinstance(t, (str, os.PathLike)):\n                if os.path.exists(t):\n                    t = torch.load(t)\n                else:\n                    raise OSError(f\"File {t} is not found.\")\n            loaded_transformers.append(t)\n        v = loaded_transformers\n        return v\n\n    @field_validator(\"output_format\")\n    def validate_output_format(cls, v):\n        supported_formats = [\"tensor\", \"variable\", \"raw\"]\n        if v not in supported_formats:\n            raise ValueError(f\"Unknown output format {v}, expected one of {supported_formats}.\")\n        return v\n\n    def evaluate(\n            self,\n            input_dict: dict[str, Union[InputVariable, float, torch.Tensor]],\n    ) -&gt; dict[str, Union[OutputVariable, float, torch.Tensor]]:\n        \"\"\"Evaluates model on the given input dictionary.\n\n        Args:\n            input_dict: Input dictionary on which to evaluate the model.\n\n        Returns:\n            Dictionary of output variable names to values.\n        \"\"\"\n        formatted_inputs = self._format_inputs(input_dict)\n        input_tensor = self._arrange_inputs(formatted_inputs)\n        input_tensor = self._transform_inputs(input_tensor)\n        output_tensor = self.model(input_tensor)\n        output_tensor = self._transform_outputs(output_tensor)\n        parsed_outputs = self._parse_outputs(output_tensor)\n        output_dict = self._prepare_outputs(parsed_outputs)\n        return output_dict\n\n    def random_input(self, n_samples: int = 1) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Generates random input(s) for the model.\n\n        Args:\n            n_samples: Number of random samples to generate.\n\n        Returns:\n            Dictionary of input variable names to tensors.\n        \"\"\"\n        input_dict = {}\n        for var in self.input_variables:\n            if isinstance(var, ScalarInputVariable):\n                input_dict[var.name] = var.value_range[0] + torch.rand(size=(n_samples,)) * (\n                            var.value_range[1] - var.value_range[0])\n            else:\n                torch.tensor(var.default, **self._tkwargs).repeat((n_samples, 1))\n        return input_dict\n\n    def random_evaluate(self, n_samples: int = 1) -&gt; dict[str, Union[OutputVariable, float, torch.Tensor]]:\n        \"\"\"Returns random evaluation(s) of the model.\n\n        Args:\n            n_samples: Number of random samples to evaluate.\n\n        Returns:\n            Dictionary of variable names to outputs.\n        \"\"\"\n        random_input = self.random_input(n_samples)\n        return self.evaluate(random_input)\n\n    def to(self, device: Union[torch.device, str]):\n        \"\"\"Updates the device for the model, transformers and default values.\n\n        Args:\n            device: Device on which the model will be evaluated.\n        \"\"\"\n        self.model.to(device)\n        for t in self.input_transformers + self.output_transformers:\n            if isinstance(t, torch.nn.Module):\n                t.to(device)\n        self.device = device\n\n    def insert_input_transformer(self, new_transformer: ReversibleInputTransform, loc: int):\n        \"\"\"Inserts an additional input transformer at the given location.\n\n        Args:\n            new_transformer: New transformer to add.\n            loc: Location where the new transformer shall be added to the transformer list.\n        \"\"\"\n        self.input_transformers = (self.input_transformers[:loc] + [new_transformer] +\n                                   self.input_transformers[loc:])\n\n    def insert_output_transformer(self, new_transformer: ReversibleInputTransform, loc: int):\n        \"\"\"Inserts an additional output transformer at the given location.\n\n        Args:\n            new_transformer: New transformer to add.\n            loc: Location where the new transformer shall be added to the transformer list.\n        \"\"\"\n        self.output_transformers = (self.output_transformers[:loc] + [new_transformer] +\n                                    self.output_transformers[loc:])\n\n    def update_input_variables_to_transformer(self, transformer_loc: int) -&gt; list[InputVariable]:\n        \"\"\"Returns input variables updated to the transformer at the given location.\n\n        Updated are the value ranges and default of the input variables. This allows, e.g., to add a\n        calibration transformer and to update the input variable specification accordingly.\n\n        Args:\n            transformer_loc: The location of the input transformer to adjust for.\n\n        Returns:\n            The updated input variables.\n        \"\"\"\n        x_old = {\n            \"min\": torch.tensor([var.value_range[0] for var in self.input_variables], dtype=self.dtype),\n            \"max\": torch.tensor([var.value_range[1] for var in self.input_variables], dtype=self.dtype),\n            \"default\": torch.tensor([var.default for var in self.input_variables], dtype=self.dtype),\n        }\n        x_new = {}\n        for key in x_old.keys():\n            x = x_old[key]\n            # compute previous limits at transformer location\n            for i in range(transformer_loc):\n                x = self.input_transformers[i].transform(x)\n            # untransform of transformer to adjust for\n            x = self.input_transformers[transformer_loc].untransform(x)\n            # backtrack through transformers\n            for transformer in self.input_transformers[:transformer_loc][::-1]:\n                x = transformer.untransform(x)\n            x_new[key] = x\n        updated_variables = deepcopy(self.input_variables)\n        for i, var in enumerate(updated_variables):\n            var.value_range = [x_new[\"min\"][i].item(), x_new[\"max\"][i].item()]\n            var.default = x_new[\"default\"][i].item()\n        return updated_variables\n\n    def _format_inputs(\n            self,\n            input_dict: dict[str, Union[InputVariable, float, torch.Tensor]],\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Formats values of the input dictionary as tensors.\n\n        Args:\n            input_dict: Dictionary of input variable names to values.\n\n        Returns:\n            Dictionary of input variable names to tensors.\n        \"\"\"\n        # NOTE: The input variable is only updated if a singular value is given (ambiguous otherwise)\n        formatted_inputs = {}\n        for var_name, var in input_dict.items():\n            if isinstance(var, InputVariable):\n                formatted_inputs[var_name] = torch.tensor(var.value, **self._tkwargs)\n                # self.input_variables[self.input_names.index(var_name)].value = var.value\n            elif isinstance(var, float):\n                formatted_inputs[var_name] = torch.tensor(var, **self._tkwargs)\n                # self.input_variables[self.input_names.index(var_name)].value = var\n            elif isinstance(var, torch.Tensor):\n                var = var.double().squeeze().to(self.device)\n                formatted_inputs[var_name] = var\n                # if var.dim() == 0:\n                #     self.input_variables[self.input_names.index(var_name)].value = var.item()\n            else:\n                TypeError(\n                    f\"Unknown type {type(var)} passed to evaluate.\"\n                    f\"Should be one of InputVariable, float or torch.Tensor.\"\n                )\n        return formatted_inputs\n\n    def _arrange_inputs(self, formatted_inputs: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"Enforces order of input variables.\n\n        Enforces the order of the input variables to be passed to the transformers and model and updates the\n        returned tensor with default values for any inputs that are missing.\n\n        Args:\n            formatted_inputs: Dictionary of input variable names to tensors.\n\n        Returns:\n            Ordered input tensor to be passed to the transformers.\n        \"\"\"\n        default_tensor = torch.tensor(\n            [var.default for var in self.input_variables], **self._tkwargs\n        )\n\n        # determine input shape\n        input_shapes = [formatted_inputs[k].shape for k in formatted_inputs.keys()]\n        if not all(ele == input_shapes[0] for ele in input_shapes):\n            raise ValueError(\"Inputs have inconsistent shapes.\")\n\n        input_tensor = torch.tile(default_tensor, dims=(*input_shapes[0], 1))\n        for key, value in formatted_inputs.items():\n            input_tensor[..., self.input_names.index(key)] = value\n\n        if input_tensor.shape[-1] != len(self.input_names):\n            raise ValueError(\n                f\"\"\"\n                Last dimension of input tensor doesn't match the expected number of inputs\\n\n                received: {default_tensor.shape}, expected {len(self.input_names)} as the last dimension\n                \"\"\"\n            )\n        return input_tensor\n\n    def _transform_inputs(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Applies transformations to the inputs.\n\n        Args:\n            input_tensor: Ordered input tensor to be passed to the transformers.\n\n        Returns:\n            Tensor of transformed inputs to be passed to the model.\n        \"\"\"\n        for transformer in self.input_transformers:\n            input_tensor = transformer.transform(input_tensor)\n        return input_tensor\n\n    def _transform_outputs(self, output_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"(Un-)Transforms the model output tensor.\n\n        Args:\n            output_tensor: Output tensor from the model.\n\n        Returns:\n            (Un-)Transformed output tensor.\n        \"\"\"\n        for transformer in self.output_transformers:\n            output_tensor = transformer.untransform(output_tensor)\n        return output_tensor\n\n    def _parse_outputs(self, output_tensor: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        \"\"\"Constructs dictionary from model output tensor.\n\n        Args:\n            output_tensor: (Un-)transformed output tensor from the model.\n\n        Returns:\n            Dictionary of output variable names to (un-)transformed tensors.\n        \"\"\"\n        parsed_outputs = {}\n        if output_tensor.dim() in [0, 1]:\n            output_tensor = output_tensor.unsqueeze(0)\n        if len(self.output_names) == 1:\n            parsed_outputs[self.output_names[0]] = output_tensor.squeeze()\n        else:\n            for idx, output_name in enumerate(self.output_names):\n                parsed_outputs[output_name] = output_tensor[..., idx].squeeze()\n        return parsed_outputs\n\n    def _prepare_outputs(\n            self,\n            parsed_outputs: dict[str, torch.Tensor],\n    ) -&gt; dict[str, Union[OutputVariable, torch.Tensor]]:\n        \"\"\"Updates and returns outputs according to output_format.\n\n        Updates the output variables within the model to reflect the new values.\n\n        Args:\n            parsed_outputs: Dictionary of output variable names to transformed tensors.\n\n        Returns:\n            Dictionary of output variable names to values depending on output_format.\n        \"\"\"\n        # for var in self.output_variables:\n        #     if parsed_outputs[var.name].dim() == 0:\n        #         idx = self.output_names.index(var.name)\n        #         if isinstance(var, ScalarOutputVariable):\n        #             self.output_variables[idx].value = parsed_outputs[var.name].item()\n        #         elif isinstance(var, ImageOutputVariable):\n        #             # OutputVariables should be numpy arrays\n        #             self.output_variables[idx].value = (parsed_outputs[var.name].reshape(var.shape).numpy())\n        #             self._update_image_limits(var, parsed_outputs)\n\n        if self.output_format == \"tensor\":\n            return parsed_outputs\n        elif self.output_format == \"variable\":\n            output_dict = {var.name: var for var in self.output_variables}\n            for var in output_dict.values():\n                var.value = parsed_outputs[var.name].item()\n            return output_dict\n            # return {var.name: var for var in self.output_variables}\n        else:\n            return {key: value.item() if value.squeeze().dim() == 0 else value\n                    for key, value in parsed_outputs.items()}\n            # return {var.name: var.value for var in self.output_variables}\n\n    def _update_image_limits(\n            self,\n            variable: OutputVariable, predicted_output: dict[str, torch.Tensor],\n    ):\n        output_idx = self.output_names.index(variable.name)\n        if self.output_variables[output_idx].x_min_variable:\n            self.output_variables[output_idx].x_min = predicted_output[\n                self.output_variables[output_idx].x_min_variable\n            ].item()\n\n        if self.output_variables[output_idx].x_max_variable:\n            self.output_variables[output_idx].x_max = predicted_output[\n                self.output_variables[output_idx].x_max_variable\n            ].item()\n\n        if self.output_variables[output_idx].y_min_variable:\n            self.output_variables[output_idx].y_min = predicted_output[\n                self.output_variables[output_idx].y_min_variable\n            ].item()\n\n        if self.output_variables[output_idx].y_max_variable:\n            self.output_variables[output_idx].y_max = predicted_output[\n                self.output_variables[output_idx].y_max_variable\n            ].item()\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes TorchModel.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Accepts a single argument which is the model configuration as dictionary, YAML or JSON formatted string or file path.</p> <code>()</code> <code>**kwargs</code> <p>See class attributes.</p> <code>{}</code> Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes TorchModel.\n\n    Args:\n        *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n          formatted string or file path.\n        **kwargs: See class attributes.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n\n    # set precision\n    self.model.to(dtype=self.dtype)\n    for t in self.input_transformers + self.output_transformers:\n        if isinstance(t, torch.nn.Module):\n            t.to(dtype=self.dtype)\n\n    # fixed model: set full model in eval mode and deactivate all gradients\n    if self.fixed_model:\n        self.model.eval().requires_grad_(False)\n        for t in self.input_transformers + self.output_transformers:\n            if isinstance(t, torch.nn.Module):\n                t.eval().requires_grad_(False)\n\n    # ensure consistent device\n    self.to(self.device)\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.evaluate","title":"<code>evaluate(input_dict)</code>","text":"<p>Evaluates model on the given input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict[str, Union[InputVariable, float, Tensor]]</code> <p>Input dictionary on which to evaluate the model.</p> required <p>Returns:</p> Type Description <code>dict[str, Union[OutputVariable, float, Tensor]]</code> <p>Dictionary of output variable names to values.</p> Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def evaluate(\n        self,\n        input_dict: dict[str, Union[InputVariable, float, torch.Tensor]],\n) -&gt; dict[str, Union[OutputVariable, float, torch.Tensor]]:\n    \"\"\"Evaluates model on the given input dictionary.\n\n    Args:\n        input_dict: Input dictionary on which to evaluate the model.\n\n    Returns:\n        Dictionary of output variable names to values.\n    \"\"\"\n    formatted_inputs = self._format_inputs(input_dict)\n    input_tensor = self._arrange_inputs(formatted_inputs)\n    input_tensor = self._transform_inputs(input_tensor)\n    output_tensor = self.model(input_tensor)\n    output_tensor = self._transform_outputs(output_tensor)\n    parsed_outputs = self._parse_outputs(output_tensor)\n    output_dict = self._prepare_outputs(parsed_outputs)\n    return output_dict\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.insert_input_transformer","title":"<code>insert_input_transformer(new_transformer, loc)</code>","text":"<p>Inserts an additional input transformer at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>new_transformer</code> <code>ReversibleInputTransform</code> <p>New transformer to add.</p> required <code>loc</code> <code>int</code> <p>Location where the new transformer shall be added to the transformer list.</p> required Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def insert_input_transformer(self, new_transformer: ReversibleInputTransform, loc: int):\n    \"\"\"Inserts an additional input transformer at the given location.\n\n    Args:\n        new_transformer: New transformer to add.\n        loc: Location where the new transformer shall be added to the transformer list.\n    \"\"\"\n    self.input_transformers = (self.input_transformers[:loc] + [new_transformer] +\n                               self.input_transformers[loc:])\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.insert_output_transformer","title":"<code>insert_output_transformer(new_transformer, loc)</code>","text":"<p>Inserts an additional output transformer at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>new_transformer</code> <code>ReversibleInputTransform</code> <p>New transformer to add.</p> required <code>loc</code> <code>int</code> <p>Location where the new transformer shall be added to the transformer list.</p> required Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def insert_output_transformer(self, new_transformer: ReversibleInputTransform, loc: int):\n    \"\"\"Inserts an additional output transformer at the given location.\n\n    Args:\n        new_transformer: New transformer to add.\n        loc: Location where the new transformer shall be added to the transformer list.\n    \"\"\"\n    self.output_transformers = (self.output_transformers[:loc] + [new_transformer] +\n                                self.output_transformers[loc:])\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.random_evaluate","title":"<code>random_evaluate(n_samples=1)</code>","text":"<p>Returns random evaluation(s) of the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of random samples to evaluate.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, Union[OutputVariable, float, Tensor]]</code> <p>Dictionary of variable names to outputs.</p> Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def random_evaluate(self, n_samples: int = 1) -&gt; dict[str, Union[OutputVariable, float, torch.Tensor]]:\n    \"\"\"Returns random evaluation(s) of the model.\n\n    Args:\n        n_samples: Number of random samples to evaluate.\n\n    Returns:\n        Dictionary of variable names to outputs.\n    \"\"\"\n    random_input = self.random_input(n_samples)\n    return self.evaluate(random_input)\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.random_input","title":"<code>random_input(n_samples=1)</code>","text":"<p>Generates random input(s) for the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of random samples to generate.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Dictionary of input variable names to tensors.</p> Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def random_input(self, n_samples: int = 1) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Generates random input(s) for the model.\n\n    Args:\n        n_samples: Number of random samples to generate.\n\n    Returns:\n        Dictionary of input variable names to tensors.\n    \"\"\"\n    input_dict = {}\n    for var in self.input_variables:\n        if isinstance(var, ScalarInputVariable):\n            input_dict[var.name] = var.value_range[0] + torch.rand(size=(n_samples,)) * (\n                        var.value_range[1] - var.value_range[0])\n        else:\n            torch.tensor(var.default, **self._tkwargs).repeat((n_samples, 1))\n    return input_dict\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.to","title":"<code>to(device)</code>","text":"<p>Updates the device for the model, transformers and default values.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[device, str]</code> <p>Device on which the model will be evaluated.</p> required Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def to(self, device: Union[torch.device, str]):\n    \"\"\"Updates the device for the model, transformers and default values.\n\n    Args:\n        device: Device on which the model will be evaluated.\n    \"\"\"\n    self.model.to(device)\n    for t in self.input_transformers + self.output_transformers:\n        if isinstance(t, torch.nn.Module):\n            t.to(device)\n    self.device = device\n</code></pre>"},{"location":"models/#lume_model.models.torch_model.TorchModel.update_input_variables_to_transformer","title":"<code>update_input_variables_to_transformer(transformer_loc)</code>","text":"<p>Returns input variables updated to the transformer at the given location.</p> <p>Updated are the value ranges and default of the input variables. This allows, e.g., to add a calibration transformer and to update the input variable specification accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>transformer_loc</code> <code>int</code> <p>The location of the input transformer to adjust for.</p> required <p>Returns:</p> Type Description <code>list[InputVariable]</code> <p>The updated input variables.</p> Source code in <code>lume_model/models/torch_model.py</code> <pre><code>def update_input_variables_to_transformer(self, transformer_loc: int) -&gt; list[InputVariable]:\n    \"\"\"Returns input variables updated to the transformer at the given location.\n\n    Updated are the value ranges and default of the input variables. This allows, e.g., to add a\n    calibration transformer and to update the input variable specification accordingly.\n\n    Args:\n        transformer_loc: The location of the input transformer to adjust for.\n\n    Returns:\n        The updated input variables.\n    \"\"\"\n    x_old = {\n        \"min\": torch.tensor([var.value_range[0] for var in self.input_variables], dtype=self.dtype),\n        \"max\": torch.tensor([var.value_range[1] for var in self.input_variables], dtype=self.dtype),\n        \"default\": torch.tensor([var.default for var in self.input_variables], dtype=self.dtype),\n    }\n    x_new = {}\n    for key in x_old.keys():\n        x = x_old[key]\n        # compute previous limits at transformer location\n        for i in range(transformer_loc):\n            x = self.input_transformers[i].transform(x)\n        # untransform of transformer to adjust for\n        x = self.input_transformers[transformer_loc].untransform(x)\n        # backtrack through transformers\n        for transformer in self.input_transformers[:transformer_loc][::-1]:\n            x = transformer.untransform(x)\n        x_new[key] = x\n    updated_variables = deepcopy(self.input_variables)\n    for i, var in enumerate(updated_variables):\n        var.value_range = [x_new[\"min\"][i].item(), x_new[\"max\"][i].item()]\n        var.default = x_new[\"default\"][i].item()\n    return updated_variables\n</code></pre>"},{"location":"models/#lume_model.models.torch_module.TorchModule","title":"<code>TorchModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper to allow a LUME TorchModel to be used like a torch.nn.Module.</p> <p>As the base model within the TorchModel is assumed to be fixed during instantiation, so is the TorchModule.</p> Source code in <code>lume_model/models/torch_module.py</code> <pre><code>class TorchModule(torch.nn.Module):\n    \"\"\"Wrapper to allow a LUME TorchModel to be used like a torch.nn.Module.\n\n    As the base model within the TorchModel is assumed to be fixed during instantiation,\n    so is the TorchModule.\n    \"\"\"\n    def __init__(\n        self,\n        *args,\n        model: TorchModel = None,\n        input_order: list[str] = None,\n        output_order: list[str] = None,\n    ):\n        \"\"\"Initializes TorchModule.\n\n        Args:\n            *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n              formatted string or file path.\n\n        Keyword Args:\n            model: The TorchModel instance to wrap around. If config is None, this has to be defined.\n            input_order: Input names in the order they are passed to the model. If None, the input order of the\n              TorchModel is used.\n            output_order: Output names in the order they are returned by the model. If None, the output order of\n              the TorchModel is used.\n        \"\"\"\n        if all(arg is None for arg in [*args, model]):\n            raise ValueError(\"Either a YAML string has to be given or model has to be defined.\")\n        super().__init__()\n        if len(args) == 1:\n            if not all(v is None for v in [model, input_order, output_order]):\n                raise ValueError(\"Cannot specify YAML string and keyword arguments for TorchModule init.\")\n            model_fields = {f\"model.{k}\": v for k, v in TorchModel.model_fields.items()}\n            kwargs = parse_config(args[0], model_fields)\n            kwargs[\"model\"] = TorchModel(kwargs[\"model\"])\n            self.__init__(**kwargs)\n        elif len(args) &gt; 1:\n            raise ValueError(\n                \"Arguments to TorchModule must be either a single YAML string or keyword arguments.\"\n            )\n        else:\n            self._model = model\n            self._input_order = input_order\n            self._output_order = output_order\n            self.register_module(\"base_model\", self._model.model)\n            for i, input_transformer in enumerate(self._model.input_transformers):\n                self.register_module(f\"input_transformers_{i}\", input_transformer)\n            for i, output_transformer in enumerate(self._model.output_transformers):\n                self.register_module(f\"output_transformers_{i}\", output_transformer)\n            if not model.model.training:  # TorchModel defines train/eval mode\n                self.eval()\n\n    @property\n    def model(self):\n        return self._model\n\n    @property\n    def input_order(self):\n        if self._input_order is None:\n            return self._model.input_names\n        else:\n            return self._input_order\n\n    @property\n    def output_order(self):\n        if self._output_order is None:\n            return self._model.output_names\n        else:\n            return self._output_order\n\n    def forward(self, x: torch.Tensor):\n        # input shape: [n_batch, n_samples, n_dim]\n        x = self._validate_input(x)\n        model_input = self._tensor_to_dictionary(x)\n        y_model = self.evaluate_model(model_input)\n        y_model = self.manipulate_output(y_model)\n        # squeeze for use as prior mean in botorch GPs\n        y = self._dictionary_to_tensor(y_model).squeeze()\n        return y\n\n    def yaml(\n            self,\n            base_key: str = \"\",\n            file_prefix: str = \"\",\n            save_models: bool = False,\n    ) -&gt; str:\n        \"\"\"Serializes the object and returns a YAML formatted string defining the TorchModule instance.\n\n        Args:\n            base_key: Base key for serialization.\n            file_prefix: Prefix for generated filenames.\n            save_models: Determines whether models are saved to file.\n\n        Returns:\n            YAML formatted string defining the TorchModule instance.\n        \"\"\"\n        d = {}\n        for k, v in inspect.signature(TorchModule.__init__).parameters.items():\n            if k not in [\"self\", \"args\", \"model\"]:\n                d[k] = getattr(self, k)\n        output = json.loads(\n            json.dumps(recursive_serialize(d, base_key, file_prefix, save_models))\n        )\n        model_output = json.loads(\n            self._model.to_json(\n                base_key=base_key,\n                file_prefix=file_prefix,\n                save_models=save_models,\n            )\n        )\n        output[\"model\"] = model_output\n        # create YAML formatted string\n        s = yaml.dump({\"model_class\": self.__class__.__name__} | output,\n                      default_flow_style=None, sort_keys=False)\n        return s\n\n    def dump(\n            self,\n            file: Union[str, os.PathLike],\n            save_models: bool = True,\n            base_key: str = \"\",\n    ):\n        \"\"\"Returns and optionally saves YAML formatted string defining the model.\n\n        Args:\n            file: File path to which the YAML formatted string and corresponding files are saved.\n            base_key: Base key for serialization.\n            save_models: Determines whether models are saved to file.\n        \"\"\"\n        file_prefix = os.path.splitext(file)[0]\n        with open(file, \"w\") as f:\n            f.write(\n                self.yaml(\n                    save_models=save_models,\n                    base_key=base_key,\n                    file_prefix=file_prefix,\n                )\n            )\n\n    def evaluate_model(self, x: dict[str, torch.Tensor]):\n        \"\"\"Placeholder method to modify model calls.\"\"\"\n        return self._model.evaluate(x)\n\n    def manipulate_output(self, y_model: dict[str, torch.Tensor]):\n        \"\"\"Placeholder method to modify the model output.\"\"\"\n        return y_model\n\n    def _tensor_to_dictionary(self, x: torch.Tensor):\n        input_dict = {}\n        for idx, input_name in enumerate(self.input_order):\n            input_dict[input_name] = x[..., idx].unsqueeze(-1)\n        return input_dict\n\n    def _dictionary_to_tensor(self, y_model: dict[str, torch.Tensor]):\n        output_tensor = torch.stack(\n            [y_model[output_name].unsqueeze(-1) for output_name in self.output_order], dim=-1\n        )\n        return output_tensor\n\n    @staticmethod\n    def _validate_input(x: torch.Tensor) -&gt; torch.Tensor:\n        if x.dim() &lt;= 1:\n            raise ValueError(\n                f\"Expected input dim to be at least 2 ([n_samples, n_features]), received: {tuple(x.shape)}\"\n            )\n        else:\n            return x\n</code></pre>"},{"location":"models/#lume_model.models.torch_module.TorchModule.__init__","title":"<code>__init__(*args, model=None, input_order=None, output_order=None)</code>","text":"<p>Initializes TorchModule.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Accepts a single argument which is the model configuration as dictionary, YAML or JSON formatted string or file path.</p> <code>()</code> <p>Other Parameters:</p> Name Type Description <code>model</code> <code>TorchModel</code> <p>The TorchModel instance to wrap around. If config is None, this has to be defined.</p> <code>input_order</code> <code>list[str]</code> <p>Input names in the order they are passed to the model. If None, the input order of the TorchModel is used.</p> <code>output_order</code> <code>list[str]</code> <p>Output names in the order they are returned by the model. If None, the output order of the TorchModel is used.</p> Source code in <code>lume_model/models/torch_module.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    model: TorchModel = None,\n    input_order: list[str] = None,\n    output_order: list[str] = None,\n):\n    \"\"\"Initializes TorchModule.\n\n    Args:\n        *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n          formatted string or file path.\n\n    Keyword Args:\n        model: The TorchModel instance to wrap around. If config is None, this has to be defined.\n        input_order: Input names in the order they are passed to the model. If None, the input order of the\n          TorchModel is used.\n        output_order: Output names in the order they are returned by the model. If None, the output order of\n          the TorchModel is used.\n    \"\"\"\n    if all(arg is None for arg in [*args, model]):\n        raise ValueError(\"Either a YAML string has to be given or model has to be defined.\")\n    super().__init__()\n    if len(args) == 1:\n        if not all(v is None for v in [model, input_order, output_order]):\n            raise ValueError(\"Cannot specify YAML string and keyword arguments for TorchModule init.\")\n        model_fields = {f\"model.{k}\": v for k, v in TorchModel.model_fields.items()}\n        kwargs = parse_config(args[0], model_fields)\n        kwargs[\"model\"] = TorchModel(kwargs[\"model\"])\n        self.__init__(**kwargs)\n    elif len(args) &gt; 1:\n        raise ValueError(\n            \"Arguments to TorchModule must be either a single YAML string or keyword arguments.\"\n        )\n    else:\n        self._model = model\n        self._input_order = input_order\n        self._output_order = output_order\n        self.register_module(\"base_model\", self._model.model)\n        for i, input_transformer in enumerate(self._model.input_transformers):\n            self.register_module(f\"input_transformers_{i}\", input_transformer)\n        for i, output_transformer in enumerate(self._model.output_transformers):\n            self.register_module(f\"output_transformers_{i}\", output_transformer)\n        if not model.model.training:  # TorchModel defines train/eval mode\n            self.eval()\n</code></pre>"},{"location":"models/#lume_model.models.torch_module.TorchModule.dump","title":"<code>dump(file, save_models=True, base_key='')</code>","text":"<p>Returns and optionally saves YAML formatted string defining the model.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, PathLike]</code> <p>File path to which the YAML formatted string and corresponding files are saved.</p> required <code>base_key</code> <code>str</code> <p>Base key for serialization.</p> <code>''</code> <code>save_models</code> <code>bool</code> <p>Determines whether models are saved to file.</p> <code>True</code> Source code in <code>lume_model/models/torch_module.py</code> <pre><code>def dump(\n        self,\n        file: Union[str, os.PathLike],\n        save_models: bool = True,\n        base_key: str = \"\",\n):\n    \"\"\"Returns and optionally saves YAML formatted string defining the model.\n\n    Args:\n        file: File path to which the YAML formatted string and corresponding files are saved.\n        base_key: Base key for serialization.\n        save_models: Determines whether models are saved to file.\n    \"\"\"\n    file_prefix = os.path.splitext(file)[0]\n    with open(file, \"w\") as f:\n        f.write(\n            self.yaml(\n                save_models=save_models,\n                base_key=base_key,\n                file_prefix=file_prefix,\n            )\n        )\n</code></pre>"},{"location":"models/#lume_model.models.torch_module.TorchModule.evaluate_model","title":"<code>evaluate_model(x)</code>","text":"<p>Placeholder method to modify model calls.</p> Source code in <code>lume_model/models/torch_module.py</code> <pre><code>def evaluate_model(self, x: dict[str, torch.Tensor]):\n    \"\"\"Placeholder method to modify model calls.\"\"\"\n    return self._model.evaluate(x)\n</code></pre>"},{"location":"models/#lume_model.models.torch_module.TorchModule.manipulate_output","title":"<code>manipulate_output(y_model)</code>","text":"<p>Placeholder method to modify the model output.</p> Source code in <code>lume_model/models/torch_module.py</code> <pre><code>def manipulate_output(self, y_model: dict[str, torch.Tensor]):\n    \"\"\"Placeholder method to modify the model output.\"\"\"\n    return y_model\n</code></pre>"},{"location":"models/#lume_model.models.torch_module.TorchModule.yaml","title":"<code>yaml(base_key='', file_prefix='', save_models=False)</code>","text":"<p>Serializes the object and returns a YAML formatted string defining the TorchModule instance.</p> <p>Parameters:</p> Name Type Description Default <code>base_key</code> <code>str</code> <p>Base key for serialization.</p> <code>''</code> <code>file_prefix</code> <code>str</code> <p>Prefix for generated filenames.</p> <code>''</code> <code>save_models</code> <code>bool</code> <p>Determines whether models are saved to file.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>YAML formatted string defining the TorchModule instance.</p> Source code in <code>lume_model/models/torch_module.py</code> <pre><code>def yaml(\n        self,\n        base_key: str = \"\",\n        file_prefix: str = \"\",\n        save_models: bool = False,\n) -&gt; str:\n    \"\"\"Serializes the object and returns a YAML formatted string defining the TorchModule instance.\n\n    Args:\n        base_key: Base key for serialization.\n        file_prefix: Prefix for generated filenames.\n        save_models: Determines whether models are saved to file.\n\n    Returns:\n        YAML formatted string defining the TorchModule instance.\n    \"\"\"\n    d = {}\n    for k, v in inspect.signature(TorchModule.__init__).parameters.items():\n        if k not in [\"self\", \"args\", \"model\"]:\n            d[k] = getattr(self, k)\n    output = json.loads(\n        json.dumps(recursive_serialize(d, base_key, file_prefix, save_models))\n    )\n    model_output = json.loads(\n        self._model.to_json(\n            base_key=base_key,\n            file_prefix=file_prefix,\n            save_models=save_models,\n        )\n    )\n    output[\"model\"] = model_output\n    # create YAML formatted string\n    s = yaml.dump({\"model_class\": self.__class__.__name__} | output,\n                  default_flow_style=None, sort_keys=False)\n    return s\n</code></pre>"},{"location":"models/#lume_model.models.keras_model.KerasModel","title":"<code>KerasModel</code>","text":"<p>               Bases: <code>LUMEBaseModel</code></p> <p>LUME-model class for keras models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Model</code> <p>The keras base model.</p> <code>output_format</code> <code>str</code> <p>Determines format of outputs: \"array\", \"variable\" or \"raw\".</p> <code>output_transforms</code> <code>list[str]</code> <p>List of strings defining additional transformations applied to the outputs. For now, only \"softmax\" is supported.</p> Source code in <code>lume_model/models/keras_model.py</code> <pre><code>class KerasModel(LUMEBaseModel):\n    \"\"\"LUME-model class for keras models.\n\n    Attributes:\n        model: The keras base model.\n        output_format: Determines format of outputs: \"array\", \"variable\" or \"raw\".\n        output_transforms: List of strings defining additional transformations applied to the outputs. For now,\n          only \"softmax\" is supported.\n    \"\"\"\n    model: keras.Model\n    output_format: str = \"array\"\n    output_transforms: list[str] = []\n\n    def __init__(\n            self,\n            *args,\n            **kwargs,\n    ):\n        \"\"\"Initializes KerasModel.\n\n        Args:\n            *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n              formatted string or file path.\n            **kwargs: See class attributes.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n    @field_validator(\"model\", mode=\"before\")\n    def validate_keras_model(cls, v):\n        if isinstance(v, (str, os.PathLike)):\n            if os.path.exists(v):\n                v = keras.models.load_model(v)\n            else:\n                raise OSError(f\"File {v} is not found.\")\n        return v\n\n    @field_validator(\"output_format\")\n    def validate_output_format(cls, v):\n        supported_formats = [\"array\", \"variable\", \"raw\"]\n        if v not in supported_formats:\n            raise ValueError(f\"Unknown output format {v}, expected one of {supported_formats}.\")\n        return v\n\n    @property\n    def dtype(self):\n        return np.double\n\n    def evaluate(\n            self,\n            input_dict: dict[str, Union[InputVariable, float, np.ndarray]],\n    ) -&gt; dict[str, Union[OutputVariable, float, np.ndarray]]:\n        \"\"\"Evaluates model on the given input dictionary.\n\n        Args:\n            input_dict: Input dictionary on which to evaluate the model.\n\n        Returns:\n            Dictionary of output variable names to values.\n        \"\"\"\n        formatted_inputs = self._format_inputs(input_dict)\n        complete_input_dict = self._complete_inputs(formatted_inputs)\n        output_array = self.model.predict(complete_input_dict).astype(self.dtype)\n        output_array = self._output_transform(output_array)\n        parsed_outputs = self._parse_outputs(output_array)\n        output_dict = self._prepare_outputs(parsed_outputs)\n        return output_dict\n\n    def random_input(self, n_samples: int = 1) -&gt; dict[str, np.ndarray]:\n        \"\"\"Generates random input(s) for the model.\n\n        Args:\n            n_samples: Number of random samples to generate.\n\n        Returns:\n            Dictionary of input variable names to arrays.\n        \"\"\"\n        input_dict = {}\n        for var in self.input_variables:\n            if isinstance(var, ScalarInputVariable):\n                input_dict[var.name] = np.random.uniform(*var.value_range, size=n_samples)\n            else:\n                default_array = np.array(var.default, dtype=self.dtype)\n                input_dict[var.name] = np.repeat(default_array.reshape((1, *default_array.shape)),\n                                                 n_samples, axis=0)\n        return input_dict\n\n    def random_evaluate(self, n_samples: int = 1) -&gt; dict[str, Union[OutputVariable, float, np.ndarray]]:\n        \"\"\"Returns random evaluation(s) of the model.\n\n        Args:\n            n_samples: Number of random samples to evaluate.\n\n        Returns:\n            Dictionary of variable names to outputs.\n        \"\"\"\n        random_input = self.random_input(n_samples)\n        return self.evaluate(random_input)\n\n    def _format_inputs(\n            self,\n            input_dict: dict[str, Union[InputVariable, float, np.ndarray]],\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Formats values of the input dictionary as arrays.\n\n        Args:\n            input_dict: Dictionary of input variable names to values.\n\n        Returns:\n            Dictionary of input variable names to arrays.\n        \"\"\"\n        # NOTE: The input variable is only updated if a singular value is given (ambiguous otherwise)\n        formatted_inputs = {}\n        for var_name, var in input_dict.items():\n            if isinstance(var, InputVariable):\n                formatted_inputs[var_name] = np.array(var.value, dtype=self.dtype)\n                # self.input_variables[self.input_names.index(var_name)].value = var.value\n            elif isinstance(var, float):\n                formatted_inputs[var_name] = np.array(var, dtype=self.dtype)\n                # self.input_variables[self.input_names.index(var_name)].value = var\n            elif isinstance(var, np.ndarray):\n                var = var.astype(self.dtype).squeeze()\n                formatted_inputs[var_name] = var\n                # if var.ndim == 0:\n                #     self.input_variables[self.input_names.index(var_name)].value = var.item()\n            else:\n                TypeError(\n                    f\"Unknown type {type(var)} passed to evaluate.\"\n                    f\"Should be one of InputVariable, float or np.ndarray.\"\n                )\n        return formatted_inputs\n\n    def _complete_inputs(self, formatted_inputs: dict[str, np.ndarray]) -&gt; dict[str, np.ndarray]:\n        \"\"\"Completes input dictionary by filling in default values.\n\n        Args:\n            formatted_inputs: Dictionary of input variable names to arrays.\n\n        Returns:\n            Completed input dictionary to be passed to the model.\n        \"\"\"\n        # determine input shape\n        input_shapes = [formatted_inputs[k].shape for k in formatted_inputs.keys()]\n        if not all(ele == input_shapes[0] for ele in input_shapes):\n            raise ValueError(\"Inputs have inconsistent shapes.\")\n\n        for i, key in enumerate(self.input_names):\n            if key not in formatted_inputs.keys():\n                default_array = np.array(self.input_variables[i].default, dtype=self.dtype)\n                formatted_inputs[key] = np.tile(default_array, reps=input_shapes[0])\n\n        if not input_shapes[0]:\n            for key in self.input_names:\n                formatted_inputs[key] = formatted_inputs[key].reshape((1, *formatted_inputs[key].shape))\n        return formatted_inputs\n\n    def _output_transform(self, output_array: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Applies additional transformations to the model output array.\n\n        Args:\n            output_array: Output array from the model.\n\n        Returns:\n            Transformed output array.\n        \"\"\"\n        if \"softmax\" in self.output_transforms:\n            output_array = np.argmax(output_array, axis=-1)\n        return output_array\n\n    def _parse_outputs(self, output_array: np.ndarray) -&gt; dict[str, np.ndarray]:\n        \"\"\"Constructs dictionary from model output array.\n\n        Args:\n            output_array: Transformed output array from the model.\n\n        Returns:\n            Dictionary of output variable names to transformed arrays.\n        \"\"\"\n        parsed_outputs = {}\n        if output_array.ndim in [0, 1]:\n            output_array = output_array.reshape((1, *output_array.shape))\n        if len(self.output_names) == 1:\n            parsed_outputs[self.output_names[0]] = output_array.squeeze()\n        else:\n            for idx, output_name in enumerate(self.output_names):\n                parsed_outputs[output_name] = output_array[..., idx].squeeze()\n        return parsed_outputs\n\n    def _prepare_outputs(\n            self,\n            parsed_outputs: dict[str, np.ndarray],\n    ) -&gt; dict[str, Union[OutputVariable, np.ndarray]]:\n        \"\"\"Updates and returns outputs according to output_format.\n\n        Updates the output variables within the model to reflect the new values.\n\n        Args:\n            parsed_outputs: Dictionary of output variable names to transformed arrays.\n\n        Returns:\n            Dictionary of output variable names to values depending on output_format.\n        \"\"\"\n        # for var in self.output_variables:\n        #     if parsed_outputs[var.name].ndim == 0:\n        #         idx = self.output_names.index(var.name)\n        #         if isinstance(var, ScalarOutputVariable):\n        #             self.output_variables[idx].value = parsed_outputs[var.name].item()\n        #         elif isinstance(var, ImageOutputVariable):\n        #             # OutputVariables should be arrays\n        #             self.output_variables[idx].value = (parsed_outputs[var.name].reshape(var.shape).numpy())\n        #             self._update_image_limits(var, parsed_outputs)\n\n        if self.output_format == \"array\":\n            return parsed_outputs\n        elif self.output_format == \"variable\":\n            output_dict = {var.name: var for var in self.output_variables}\n            for var in output_dict.values():\n                var.value = parsed_outputs[var.name].item()\n            return output_dict\n            # return {var.name: var for var in self.output_variables}\n        else:\n            return {key: value.item() if value.squeeze().ndim == 0 else value\n                    for key, value in parsed_outputs.items()}\n            # return {var.name: var.value for var in self.output_variables}\n\n    def _update_image_limits(\n            self,\n            variable: OutputVariable, predicted_output: dict[str, np.ndarray],\n    ):\n        output_idx = self.output_names.index(variable.name)\n        if self.output_variables[output_idx].x_min_variable:\n            self.output_variables[output_idx].x_min = predicted_output[\n                self.output_variables[output_idx].x_min_variable\n            ].item()\n\n        if self.output_variables[output_idx].x_max_variable:\n            self.output_variables[output_idx].x_max = predicted_output[\n                self.output_variables[output_idx].x_max_variable\n            ].item()\n\n        if self.output_variables[output_idx].y_min_variable:\n            self.output_variables[output_idx].y_min = predicted_output[\n                self.output_variables[output_idx].y_min_variable\n            ].item()\n\n        if self.output_variables[output_idx].y_max_variable:\n            self.output_variables[output_idx].y_max = predicted_output[\n                self.output_variables[output_idx].y_max_variable\n            ].item()\n</code></pre>"},{"location":"models/#lume_model.models.keras_model.KerasModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes KerasModel.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Accepts a single argument which is the model configuration as dictionary, YAML or JSON formatted string or file path.</p> <code>()</code> <code>**kwargs</code> <p>See class attributes.</p> <code>{}</code> Source code in <code>lume_model/models/keras_model.py</code> <pre><code>def __init__(\n        self,\n        *args,\n        **kwargs,\n):\n    \"\"\"Initializes KerasModel.\n\n    Args:\n        *args: Accepts a single argument which is the model configuration as dictionary, YAML or JSON\n          formatted string or file path.\n        **kwargs: See class attributes.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"models/#lume_model.models.keras_model.KerasModel.evaluate","title":"<code>evaluate(input_dict)</code>","text":"<p>Evaluates model on the given input dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict[str, Union[InputVariable, float, ndarray]]</code> <p>Input dictionary on which to evaluate the model.</p> required <p>Returns:</p> Type Description <code>dict[str, Union[OutputVariable, float, ndarray]]</code> <p>Dictionary of output variable names to values.</p> Source code in <code>lume_model/models/keras_model.py</code> <pre><code>def evaluate(\n        self,\n        input_dict: dict[str, Union[InputVariable, float, np.ndarray]],\n) -&gt; dict[str, Union[OutputVariable, float, np.ndarray]]:\n    \"\"\"Evaluates model on the given input dictionary.\n\n    Args:\n        input_dict: Input dictionary on which to evaluate the model.\n\n    Returns:\n        Dictionary of output variable names to values.\n    \"\"\"\n    formatted_inputs = self._format_inputs(input_dict)\n    complete_input_dict = self._complete_inputs(formatted_inputs)\n    output_array = self.model.predict(complete_input_dict).astype(self.dtype)\n    output_array = self._output_transform(output_array)\n    parsed_outputs = self._parse_outputs(output_array)\n    output_dict = self._prepare_outputs(parsed_outputs)\n    return output_dict\n</code></pre>"},{"location":"models/#lume_model.models.keras_model.KerasModel.random_evaluate","title":"<code>random_evaluate(n_samples=1)</code>","text":"<p>Returns random evaluation(s) of the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of random samples to evaluate.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, Union[OutputVariable, float, ndarray]]</code> <p>Dictionary of variable names to outputs.</p> Source code in <code>lume_model/models/keras_model.py</code> <pre><code>def random_evaluate(self, n_samples: int = 1) -&gt; dict[str, Union[OutputVariable, float, np.ndarray]]:\n    \"\"\"Returns random evaluation(s) of the model.\n\n    Args:\n        n_samples: Number of random samples to evaluate.\n\n    Returns:\n        Dictionary of variable names to outputs.\n    \"\"\"\n    random_input = self.random_input(n_samples)\n    return self.evaluate(random_input)\n</code></pre>"},{"location":"models/#lume_model.models.keras_model.KerasModel.random_input","title":"<code>random_input(n_samples=1)</code>","text":"<p>Generates random input(s) for the model.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Number of random samples to generate.</p> <code>1</code> <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>Dictionary of input variable names to arrays.</p> Source code in <code>lume_model/models/keras_model.py</code> <pre><code>def random_input(self, n_samples: int = 1) -&gt; dict[str, np.ndarray]:\n    \"\"\"Generates random input(s) for the model.\n\n    Args:\n        n_samples: Number of random samples to generate.\n\n    Returns:\n        Dictionary of input variable names to arrays.\n    \"\"\"\n    input_dict = {}\n    for var in self.input_variables:\n        if isinstance(var, ScalarInputVariable):\n            input_dict[var.name] = np.random.uniform(*var.value_range, size=n_samples)\n        else:\n            default_array = np.array(var.default, dtype=self.dtype)\n            input_dict[var.name] = np.repeat(default_array.reshape((1, *default_array.shape)),\n                                             n_samples, axis=0)\n    return input_dict\n</code></pre>"},{"location":"utils/","title":"Utilities","text":""},{"location":"utils/#lume_model.utils.variables_as_yaml","title":"<code>variables_as_yaml(input_variables, output_variables, file=None)</code>","text":"<p>Returns and optionally saves YAML formatted string defining the in- and output variables.</p> <p>Parameters:</p> Name Type Description Default <code>input_variables</code> <code>list[InputVariable]</code> <p>List of input variables.</p> required <code>output_variables</code> <code>list[OutputVariable]</code> <p>List of output variables.</p> required <code>file</code> <code>Union[str, PathLike]</code> <p>If not None, YAML formatted string is saved to given file path.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>YAML formatted string defining the in- and output variables.</p> Source code in <code>lume_model/utils.py</code> <pre><code>def variables_as_yaml(\n        input_variables: list[InputVariable],\n        output_variables: list[OutputVariable],\n        file: Union[str, os.PathLike] = None,\n) -&gt; str:\n    \"\"\"Returns and optionally saves YAML formatted string defining the in- and output variables.\n\n    Args:\n        input_variables: List of input variables.\n        output_variables: List of output variables.\n        file: If not None, YAML formatted string is saved to given file path.\n\n    Returns:\n        YAML formatted string defining the in- and output variables.\n    \"\"\"\n    for variables in [input_variables, output_variables]:\n        verify_unique_variable_names(variables)\n    v = {\"input_variables\": [var.dict() for var in input_variables],\n         \"output_variables\": [var.dict() for var in output_variables]}\n    s = yaml.dump(serialize_variables(v), default_flow_style=None, sort_keys=False)\n    if file is not None:\n        with open(file, \"w\") as f:\n            f.write(s)\n    return s\n</code></pre>"},{"location":"utils/#lume_model.utils.variables_from_dict","title":"<code>variables_from_dict(config)</code>","text":"<p>Parses given config and returns in- and output variable lists.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Variable configuration.</p> required <p>Returns:</p> Type Description <code>tuple[list[InputVariable], list[OutputVariable]]</code> <p>In- and output variable lists.</p> Source code in <code>lume_model/utils.py</code> <pre><code>def variables_from_dict(config: dict) -&gt; tuple[list[InputVariable], list[OutputVariable]]:\n    \"\"\"Parses given config and returns in- and output variable lists.\n\n    Args:\n        config: Variable configuration.\n\n    Returns:\n        In- and output variable lists.\n    \"\"\"\n    input_variables, output_variables = [], []\n    for key, value in {**config}.items():\n        if key in [\"input_variables\", \"output_variables\"]:\n            for var in value:\n                variable_type = var.get(\"variable_type\", var.get(\"type\"))\n                if variable_type == \"scalar\":\n                    if key == \"input_variables\":\n                        input_variables.append(ScalarInputVariable(**var))\n                    elif key == \"output_variables\":\n                        output_variables.append(ScalarOutputVariable(**var))\n                elif variable_type in [\"array\", \"image\"]:\n                    raise ValueError(f\"Parsing of variable type {variable_type} is not yet implemented.\")\n                else:\n                    raise ValueError(f\"Unknown variable type {variable_type}.\")\n    for variables in [input_variables, output_variables]:\n        verify_unique_variable_names(variables)\n    return input_variables, output_variables\n</code></pre>"},{"location":"utils/#lume_model.utils.variables_from_yaml","title":"<code>variables_from_yaml(yaml_obj)</code>","text":"<p>Parses YAML object and returns in- and output variable lists.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_obj</code> <code>Union[str, PathLike]</code> <p>YAML formatted string or file path.</p> required <p>Returns:</p> Type Description <code>tuple[list[InputVariable], list[OutputVariable]]</code> <p>In- and output variable lists.</p> Source code in <code>lume_model/utils.py</code> <pre><code>def variables_from_yaml(yaml_obj: Union[str, os.PathLike]) -&gt; tuple[list[InputVariable], list[OutputVariable]]:\n    \"\"\"Parses YAML object and returns in- and output variable lists.\n\n    Args:\n        yaml_obj: YAML formatted string or file path.\n\n    Returns:\n        In- and output variable lists.\n    \"\"\"\n    if os.path.exists(yaml_obj):\n        with open(yaml_obj) as f:\n            yaml_str = f.read()\n    else:\n        yaml_str = yaml_obj\n    config = deserialize_variables(yaml.safe_load(yaml_str))\n    return variables_from_dict(config)\n</code></pre>"},{"location":"variables/","title":"Variables","text":"<p>This module contains definitions of LUME-model variables for use with lume tools. The variables are divided into input and outputs, each with different minimal requirements. Initiating any variable without the minimum requirements will result in an error.</p> <p>For now, only scalar variables (floats) are supported.</p>"},{"location":"variables/#lume_model.variables.Variable","title":"<code>Variable</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[Value]</code></p> <p>Minimum requirements for a variable.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the variable.</p> <code>value</code> <code>Optional[Value]</code> <p>Value assigned to the variable.</p> <code>precision</code> <code>Optional[int]</code> <p>Precision to use for the value.</p> Source code in <code>lume_model/variables.py</code> <pre><code>class Variable(BaseModel, Generic[Value]):\n    \"\"\"\n    Minimum requirements for a variable.\n\n    Attributes:\n        name: Name of the variable.\n        value: Value assigned to the variable.\n        precision: Precision to use for the value.\n    \"\"\"\n    name: str\n    value: Optional[Value] = None\n    precision: Optional[int] = None\n</code></pre>"},{"location":"variables/#lume_model.variables.ScalarVariable","title":"<code>ScalarVariable</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class used for constructing a scalar variable.</p> <p>Attributes:</p> Name Type Description <code>variable_type</code> <code>str</code> <p>Indicates scalar variable.</p> <code>units</code> <code>Optional[str]</code> <p>Units associated with scalar value.</p> <code>parent_variable</code> <code>str</code> <p>Variable for which this is an attribute.</p> Source code in <code>lume_model/variables.py</code> <pre><code>class ScalarVariable(BaseModel):\n    \"\"\"\n    Base class used for constructing a scalar variable.\n\n    Attributes:\n        variable_type: Indicates scalar variable.\n        units: Units associated with scalar value.\n        parent_variable: Variable for which this is an attribute.\n    \"\"\"\n    variable_type: str = \"scalar\"\n    units: Optional[str] = None  # required for some output displays\n    parent_variable: str = (\n        None  # indicates that this variable is an attribute of another\n    )\n</code></pre>"},{"location":"variables/#lume_model.variables.InputVariable","title":"<code>InputVariable</code>","text":"<p>               Bases: <code>Variable</code>, <code>Generic[Value]</code></p> <p>Base class for input variables.</p> <p>Attributes:</p> Name Type Description <code>default</code> <code>Value</code> <p>Default value assigned to the variable.</p> <code>is_constant</code> <code>bool</code> <p>Indicates whether the variable is constant.</p> Source code in <code>lume_model/variables.py</code> <pre><code>class InputVariable(Variable, Generic[Value]):\n    \"\"\"\n    Base class for input variables.\n\n    Attributes:\n        default: Default value assigned to the variable.\n        is_constant: Indicates whether the variable is constant.\n    \"\"\"\n    default: Value  # required default\n    is_constant: bool = Field(False)\n</code></pre>"},{"location":"variables/#lume_model.variables.OutputVariable","title":"<code>OutputVariable</code>","text":"<p>               Bases: <code>Variable</code>, <code>Generic[Value]</code></p> <p>Base class for output variables. Value and range assignment are optional.</p> <p>Attributes:</p> Name Type Description <code>default</code> <code>Optional[Value]</code> <p>Default value assigned to the variable.</p> <code>value_range</code> <code>Optional[list]</code> <p>Acceptable range for value.</p> Source code in <code>lume_model/variables.py</code> <pre><code>class OutputVariable(Variable, Generic[Value]):\n    \"\"\"\n    Base class for output variables. Value and range assignment are optional.\n\n    Attributes:\n        default: Default value assigned to the variable.\n        value_range: Acceptable range for value.\n    \"\"\"\n    default: Optional[Value] = None\n    value_range: Optional[list] = Field(None, alias=\"range\")\n</code></pre>"},{"location":"variables/#lume_model.variables.ScalarInputVariable","title":"<code>ScalarInputVariable</code>","text":"<p>               Bases: <code>InputVariable[float]</code>, <code>ScalarVariable</code></p> <p>Variable used for representing a scalar input. Scalar variables hold float values. Initialization requires name, default, and value_range.</p> <p>Attributes:</p> Name Type Description <code>value_range</code> <code>list[float]</code> <p>Acceptable range for value.</p> Example <pre><code>variable = ScalarInputVariable(name=\"example_input\", default=0.1, value_range=[0.0, 1.0])\n</code></pre> Source code in <code>lume_model/variables.py</code> <pre><code>class ScalarInputVariable(InputVariable[float], ScalarVariable):\n    \"\"\"\n    Variable used for representing a scalar input. Scalar variables hold float values.\n    Initialization requires name, default, and value_range.\n\n    Attributes:\n        value_range: Acceptable range for value.\n\n    Example:\n        ```\n        variable = ScalarInputVariable(name=\"example_input\", default=0.1, value_range=[0.0, 1.0])\n        ```\n    \"\"\"\n    value_range: list[float]\n</code></pre>"},{"location":"variables/#lume_model.variables.ScalarOutputVariable","title":"<code>ScalarOutputVariable</code>","text":"<p>               Bases: <code>OutputVariable[float]</code>, <code>ScalarVariable</code></p> <p>Variable used for representing a scalar output. Scalar variables hold float values. Initialization requires name.</p> Example <pre><code>variable = ScalarOutputVariable(name=\"example_output\")\n</code></pre> Source code in <code>lume_model/variables.py</code> <pre><code>class ScalarOutputVariable(OutputVariable[float], ScalarVariable):\n    \"\"\"\n    Variable used for representing a scalar output. Scalar variables hold float values.\n    Initialization requires name.\n\n    Example:\n        ```\n        variable = ScalarOutputVariable(name=\"example_output\")\n        ```\n    \"\"\"\n    pass\n</code></pre>"}]}